{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pygame\n",
    "import sys\n",
    "from PIL import Image\n",
    "from IPython import display as ipythonDisplay\n",
    "#해당 그래프를 인라인으로 표시\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cartpole \n",
    "state  : [위치, 속도, 각, 각속도]\n",
    "action : left(-1), right(+1)\n",
    "reward : sim이 끝이 나지 않는 한 계속 +1\n",
    "1) pole이 수직으로부터 15도 이상 기울어질 때\n",
    "2) cart가 중심으로부터 2.4 만큼 떨어질 때\n",
    "episode가 끝난다.\n",
    "'''\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "# state : 시작할 때의 값(초기화)\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CHR\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "c:\\Users\\CHR\\anaconda3\\Lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "reward : 이전 action에 의해서 얻은 보상의 총합 -> 항상 학습은 reward를 증가시키는 방향으로\n",
    "observation : 관찰에 따른 구체적인 요소들의 array\n",
    "done : episode가 끝났을 경우 true, 끝나지 않았다면 false\n",
    "현 예제에서는 reward = 1 <- done == false\n",
    "**done = true 이후 reward를 0 반환\n",
    "'''\n",
    "#100번의 time step\n",
    "for _ in range(100):\n",
    "    #visualize\n",
    "    env.render()\n",
    "    #random한 action\n",
    "    action = env.action_space.sample()    \n",
    "    #action을 취했을 때의 결과값\n",
    "    observation, reward, done, _, info = env.step(action)    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 28 timesteps\n",
      "Episode finished after 10 timesteps\n"
     ]
    }
   ],
   "source": [
    "#episode 2번 진행\n",
    "for i_episode in range(2):\n",
    "    observation = env.reset()\n",
    "    for k in range(100):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, _, info = env.step(action)\n",
    "        #done이 true가 됐을 경우 break\n",
    "        if done :\n",
    "            print(\"Episode finished after {} timesteps\".format(k+1))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins_pos = 10\n",
    "n_bins_vel = 10\n",
    "n_bins_ang = 10\n",
    "n_bins_anv = 10\n",
    "\n",
    "'''\n",
    "문제상황\n",
    "1. 상태는 연속적이지 않음 s0, s1, s2.....\n",
    "2. 그러나 실제 문제를 해결하는 상황에서는 state가 연속적인 경우가 많음\n",
    "3. 따라서 연속적인 상태를 처리하기 위해서 연속적인 state를 양자화 시켜주는 구문이 필요함\n",
    "'''\n",
    "# 연속적인 상태를 discrete하게 변환해주는 함수\n",
    "def map_discrete_state(state):\n",
    "    #state를 unpacking\n",
    "    pos, vel, ang, anv = state\n",
    "    #histogram : \n",
    "    '''\n",
    "    NumPy의 np.histogram() 함수의 출력 형식은 두 가지 배열로 구성된 튜플(tuple)입니다:\n",
    "        첫 번째 배열은 각 bin의 빈도수를 포함합니다.\n",
    "        두 번째 배열은 각 bin의 경계값을 포함합니다.\n",
    "    np.where() 함수는 조건을 만족하는 요소의 인덱스를 반환합니다. \n",
    "    이때, 반환되는 값은 튜플(tuple) 형태입니다. \n",
    "    튜플의 첫 번째 요소는 조건을 만족하는 요소의 행(row) 인덱스들의 배열이고, \n",
    "    두 번째 요소는 조건을 만족하는 요소의 열(column) 인덱스들의 배열입니다.\n",
    "    '''\n",
    "    #clip : 들어온 값이 설정된 값(-2, 2) 이상이라면 한계값으로 설정\n",
    "    #bins : 들어온 값으로 range를 등분\n",
    "    #이후 1이 되는 idx를 반환\n",
    "    idx_pos = np.where(np.histogram(np.clip(pos,-2,2), bins = n_bins_pos, range = (-2,2))[0] == 1)[0][0]\n",
    "    idx_vel = np.where(np.histogram(np.clip(vel,-2,2), bins = n_bins_vel, range = (-2,2))[0] == 1)[0][0]\n",
    "    idx_ang = np.where(np.histogram(np.clip(ang,-0.4,0.4), bins = n_bins_ang, range = (-0.4,0.4))[0] == 1)[0][0]\n",
    "    idx_anv = np.where(np.histogram(np.clip(anv,-2,2), bins = n_bins_anv, range = (-2,2))[0] == 1)[0][0]\n",
    "    '''\n",
    "     따라서 states 배열의 특정 위치에 1을 할당함으로써 해당하는 상태를 나타내는 원-핫 인코딩(one-hot encoding) 벡터를 생성합니다. \n",
    "     최종적으로 reshape(-1,1)을 사용하여 states 배열을 1차원 배열로 변형합니다.\n",
    "    '''\n",
    "\n",
    "    states = np.zeros([n_bins_pos, n_bins_vel, n_bins_ang, n_bins_anv])\n",
    "    states[idx_pos, idx_vel, idx_ang, idx_anv] = 1\n",
    "    #-1로 reshape되었으므로 배열의 모양이 자동으로 조정되었을 것입니다.\n",
    "    states = states.reshape(-1,1)\n",
    "\n",
    "    s = np.where(states == 1)[0][0]\n",
    "    \n",
    "    return s  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 steps: 9\n",
      "Episode: 100 steps: 31\n",
      "Episode: 200 steps: 75\n",
      "Episode: 300 steps: 113\n",
      "Episode: 400 steps: 114\n",
      "Episode: 500 steps: 136\n",
      "Episode: 600 steps: 168\n",
      "Episode: 700 steps: 162\n",
      "Episode: 800 steps: 157\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.3\n",
    "gamma = 0.9\n",
    "n_states = n_bins_pos*n_bins_vel*n_bins_ang*n_bins_anv\n",
    "n_actions = 2\n",
    "# 0과 1 사이의 값으로 Q_table을 초기화 시켜줌\n",
    "Q_table = np.random.uniform(0, 1, (n_states, n_actions))\n",
    "#생성 시키는 episode의 개수\n",
    "for episode in range(801):\n",
    "    \n",
    "    done = False\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        count += 1        \n",
    "        s = map_discrete_state(state)    \n",
    "\n",
    "        # Exploration vs. Exploitation for action       \n",
    "        epsilon = 0.1\n",
    "        '''\n",
    "        입실론 - 탐욕법을 활용하여 0과 1 사이를 랜덤하게 뽑은 값이\n",
    "        입실론 값보다 작다면 탐험을,\n",
    "        입실론 값보다 크다면 활용을 한다.\n",
    "        '''\n",
    "        # 탐험\n",
    "        if np.random.uniform() < epsilon:\n",
    "            #가능한 action 중에서 랜덤하게 하나를 뽑는다.\n",
    "            a = env.action_space.sample()\n",
    "        # 활용\n",
    "        else:\n",
    "            # action 중 Q table에서 그 값을 최대로 하는 action을 선택한다.\n",
    "            a = np.argmax(Q_table[s, :])                \n",
    "\n",
    "        # next state and reward\n",
    "        next_state, reward, done, _, _ = env.step(a) \n",
    "                \n",
    "        if done:        \n",
    "            reward = -100        \n",
    "            Q_table[s, a] = reward\n",
    "                 \n",
    "        else:                                               \n",
    "            # Temporal Difference Update\n",
    "            '''\n",
    "            TD를 이용해서 smooth하게 Q_table을 update를 해줌\n",
    "            '''\n",
    "            next_s = map_discrete_state(next_state)\n",
    "            Q_table[s, a] = (1 - alpha)*Q_table[s, a] + alpha*(reward + gamma*np.max(Q_table[next_s, :]))\n",
    "\n",
    "        state = next_state\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode: {} steps: {}\".format(episode, count))\n",
    "        \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "cnt = 0\n",
    "env.render()\n",
    "screen = env.render()\n",
    "images = [Image.fromarray(screen)]\n",
    "while not done:\n",
    "    if cnt % 10 == 0:\n",
    "        screen = env.render()\n",
    "        images.append(Image.fromarray(screen))\n",
    "    s = map_discrete_state(state)\n",
    "    a = np.argmax(Q_table[s,:])\n",
    "    \n",
    "    next_state, _, done, _, _ = env.step(a)      \n",
    "    state = next_state  \n",
    "    cnt +=1  \n",
    "env.close()\n",
    "\n",
    "image_file = 'cartpole-v1.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
